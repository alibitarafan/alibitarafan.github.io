# Site
repository: alibitarafan/alibitarafa.github.io


# Content configuration version
version: 2

# Personal info
name: Ali Bitarafan
title: Senior Data Engineer | Data Architect
email: ali.bitarafan@gmail.com
# email_title: Email (Email title override)
# phone: Your phone number (optional)
# phone_title: Phone (Phone title override)
# website: Your website (eg. https://google.com)(optional)
# website_title: Web (Website title override)

# Dark Mode (true/false/never)
darkmode: false

# Social links
# twitter_username: jekyllrb
# github_username:  jekyll
# stackoverflow_username: "00000001"
# dribbble_username: jekyll
# facebook_username: jekyll
# flickr_username: jekyll
# instagram_username: jekyll
# linkedin_username: ali-bitarafan
# xing_username: jekyll
# pinterest_username: jekyll
# youtube_username: jekyll
# orcid_username: 0000-0000-0000-0000
# googlescholar_username: D847cGsAAAAJ

# Additional icon links
additional_links:
  - title: Solutions Architect Essentials 
    url: https://www.credential.net/d3129814-e75f-4da2-8f4b-3b832f80d43b
    image: /images/DB-SAE.png
  - title: Data Bricks Lakehouse fundamentals
    url: https://credentials.databricks.com/509bb8d3-46f5-41b1-93b6-3de27b59e65a
    image: /images/DB-lakehouse-fundamentals.png
  - title: Snow Pro - Core
    url: https://achieve.snowflake.com/ef5ec5c1-5437-4cba-ab60-aaa34e305c96
    image: /images/snowpro-core.png
  - title: Data Bricks Generative AI fundamentals
    url: https://credentials.databricks.com/1e574d0d-316b-4758-b07b-cf307336c474
    image: /images/DB-generative-ai.png
  - title: Azure Data Engineer Associate
    url: https://learn.microsoft.com/en-us/users/alibitarafan-22/credentials/e048254457227f56
    image: /images/microsoft-certified-associate-badge.svg
  - title: AWS Certified Solutions Architect - associate
    url: https://cp.certmetrics.com/amazon/en/public/verify/credential/9BCLZX7KW2RE1NKH
    image: /images/AWS-Solutions-Architect-Associate_badge.png
# - title: Link name
#   icon: Font Awesome brand icon name (eg. fab fa-twitter) (https://fontawesome.com/icons?d=gallery&s=brands&m=free)
#   url: Link url (eg. https://google.com)
# - title: another link
#   icon: font awesome brand icon name (eg. fab fa-twitter) (https://fontawesome.com/icons?d=gallery&s=brands&m=free)
#   url: Link url (eg. https://google.com)



# Google Analytics and Tag Manager
# Using more than one of these may cause issues with reporting
# gtm: "GTM-0000000"
# gtag: "UA-00000000-0"
# google_analytics: "UA-00000000-0"

# About Section
# about_title: About Me (Use this to override about section title)
about_profile_image: images/self-portrait.JPG
about_content: | # this will include new lines to allow paragraphs
    My background lies in data engineering and architecture, where I have honed my skills in 
    creating <mark>data warehouse</mark> solutions, managing <mark>data pipelines</mark>, and 
    implementing data governance. Teamwork and continuous learning are values that resonate 
    with me, allowing me to adapt to new technologies and methodologies. Effective communication 
    is one of my key strengths, enabling successful collaboration with business stakeholders to 
    translate their requirements into technical solutions. This combination of expertise in data 
    management and strong communication abilities positions me well for roles such as Data Engineering and  
    Data Architect. Additionally, my passion for data and technology fuels
    my drive to remain current in the field. 

#  Write an awesome description about yourself here, this supports markdown, so you can add [links](http://foobar.com) and highlight things <mark>like this</mark>.
#
#  You can even add paragraphs by using empty lines like this and add anything else [markdown](https://www.markdownguide.org/getting-started#what-is-markdown) supports such as
#    - Lists
#    - Tables
# - <a href="google.com">Links</a>
# - Images ![alt text](/images/landscape-trees.jpg "Trees")


content:
  - title: Projects # Title for the section
    layout: list # Type of content section (list/text)
    content:
    - layout: left
      border: weak  # Value of `weak` will display a weak border below this item. # Any 
                    # other value (or no value) means no border will be displayed
      title: Konoike Transport
      # link: 
      # link_text: 
      quote: >
        Storage and Picking Optimization Project
      description: | # this will include new lines to allow paragraphs
        In this project, I worked as data architect and tech lead to communicate and lead implementation of two AI solutions with the aim of optimizing storage and picking processes. 
        This project involved working with business stakeholders to understand their requirements 
        and translate them into technical solutions.
    - layout: left
      border: weak  # Value of `weak` will display a weak border below this item. # Any 
                    # other value (or no value) means no border will be displayed
      title: Governmental Organisation
      # link: 
      # link_text: 
      quote: >
        Optimising and modernising the data warehouse and data pipelines
      description: | # this will include new lines to allow paragraphs
        While keeping the legacy data warehouse running, I developed and implemented a modern data warehouse
        in Microsoft SQL Server. I also improved existing <mark>Data Vault</mark> models and developed data 
        pipelines to extract, transform and load data from various sources into the new data warehouse.  
      
    - layout: left
      border: weak  # Value of `weak` will display a weak border below this item. # Any 
                    # other value (or no value) means no border will be displayed
      title: Forest bi-products manufacturer
      quote: >
        Optimizing the manufacturing reporting process
      description: | # this will include new lines to allow paragraphs
        In this project I developed and collaboratively designed data vault model of the manufacturing 
        process to optimize the reporting process. Furthermore I developed data pipelines to extract, 
        transform and load data from legacy data warehouse into Azure SQL Server while taking advantage 
        of Cloud-based Modern dataOps mechanisms. 
    - layout: left
      border: weak
      title: Steel manufacturer
      quote: >
        Data catalog and data governance of business analytics and industrial data
      description: | # this will include new lines to allow paragraphs
        Communicated with business stakeholders to understand the data governance requirements. 
        Implemented Microsoft Purview to catalog the data sources and data assets of the company. 
        While implementing a wide range of cataloging and data governance features, I also 
        maintained security requirements and data privacy regulations.

    - layout: left
      border: weak
      title: Pharmceutical
      quote: >
        Data migration from on premise  to Cloud
      description: | # this will include new lines to allow paragraphs
        I was responsible for Data migration strategy and execution part of which included carrying
        out AWS RDS Oracle procedure design and execution. I managed to migrate multiple partitions and 
        tables in respect to database version changes. During this project I took advantage AWS Glue and
        Athena to explore migrated loads and AWS CDK to deploy CI/CD pipelines.

  - title: Experience
    layout: list
    content:
      - layout: right
        border: weak
        title: Etteplan
        sub_title: Senior data engineer
        caption: April 2024 - Present
        link: https://www.etteplan.com
        quote: >
          Etteplan is a global company that specializes in AI as well as other industrial engineering services
        description: | # this will include new lines to allow paragraphs
          As a senior data engineer, I am responsible for designing and implementing data workloads for AI solutions.
          I have practiced data engineering best practices and have gained experience in data modeling, data pipelines, 
          and data governance. I have worked on projects that involve logistic process automation, data science, and data analytics.
      - layout: right
        border: weak
        title: Lease Deal it
        sub_title: Senior Consultant
        caption: December 2023 - April 2024
        link: https://www.leasedealit.fi
        quote: >
          Lease deal it is a house of experts in the field of Information technology and business intelligence
        description: | # this will include new lines to allow paragraphs
          As a senior consultant, I am accountable for effectively communicating with clients to 
          understand their requirements and implementing customized data warehouse solutions. 
          My role encompasses the design and implementation of data pipelines, including building 
          <mark>SQL procedures</mark>, resolving issues discovered within existing data pipelines, engaging in 
          discussions on data modeling, and optimizing existing data pipelines for improved efficiency.
      - layout: right
        title: Solita Oy
        border: weak
        sub_title: Data Engineer
        caption: April 2022 - December 2023
        link: https://www.solita.fi
        quote: >
          IT consulting company with a focus on data and analytics
        description: | # this will include new lines to allow paragraphs
          In my role as a Data Engineer, I take pride in solving complex problems and have gained 
          experience in data modeling as well as developing data pipelines using both AWS and 
          Microsoft Azure platforms. I have employed Agile development methodologies to build 
          robust and resilient data pipelines that adhere to DevOps best practices. My proficiency 
          includes working with various tools, frameworks, and methods such as <mark>Azure DevOps</mark>, 
          Datalake, Data Vault, Databricks DE/MLops, Snowflake, <mark>AWS/Azure CLI</mark>, AWS Lambda/RDS/S3/Glue, 
          AWS/Azure datalake, and MS Purview. I have hands-on experience with Azure Data Factory 
          to optimize existing data pipelines and improve overall efficiency.       
      - layout: right
        title: Kanava.to
        border: weak
        sub_title: Full Stack Developer
        caption: May 2021 - Mar 2022
        link: https://www.kanava.to
        quote: >
          Marketing and advertising company with a focus on digital media and application development
        description: | # this will include new lines to allow paragraphs
          I successfully built eCommerce applications and incorporated business
          intelligence features through proficiency in various programming languages 
          such as nodeJs, VueJs, and PHP, as well as databases like MySQL and PostgreSQL. 
          In this role, I am responsible for reporting progress to stakeholders, 
          engaging in development discussions and planning, facilitating database migration, 
          and executing web application renovations.
      - layout: right
        title: Centria University of Applied Sciences
        sub_title: R&D Engineer
        caption: Aug 2017 - Dec 2020
        link: https://www.centria.fi
        quote: >
          Higher education institution with focus on research and development
        description: | # this will include new lines to allow paragraphs
          I developed numerous automation tools and prepared quality data along with analytical reports that contributed to significant managerial decisions. I was granted the opportunity to share my learnings in business intelligence and ERP solutions like SAP BW/4HANA, SAP S/4HANA, and Analytical applications such as SAP Analytics Cloud with Bachelor's and Master's degree students. In this capacity, I practiced team coaching, tech-leadership, working extensively with <mark>SAP BW/4HANA</mark>, SAP S/4HANA, <mark>SAP Analytics Cloud</mark>, SAP Smart data integration, and employed Python and shell scripting for efficient problem-solving.
  - title: Education
    layout: list
    content:
      - layout: top-right
        border: weak
        title: Åbo Akademi
        sub_title: Master of Computer Science
        caption: 2025
        quote: >
          With focus on AI and machine learning, My major was Data science and I wrote my thesis on
          Automated <mark>knowledge extraction</mark> from text using LLMs.
        description: | # this will include new lines to allow paragraphs
        
      - layout: top-right
        title: Centria University of Applied Sciences
        sub_title: Bachelor of Information Technology
        caption: 2019
        quote: >
          I learned principles of software development and application engineering. I began to shift towards data engineering along the way and focused on writing my thesis about <mark>data integration</mark> in data warehousing and business intelligence applications. 
        description:
  
  # - title: A Little More About Me
  #   layout: text
  #   content: | # this will include new lines to allow paragraphs
  #     This is where you can write a little more about yourself. You could title this section **Interests** and include some of your other interests.

  #     Or you could title it **Skills** and write a bit more about things that make you more desirable, like *leadership* or *teamwork*

# Footer
footer_show_references: true
# references_title: References on request (Override references text)

# Build settings
# theme: modern-resume-theme (Use this is you are hosting your resume yourself)
# remote_theme: sproogen/modern-resume-theme (Use this if you are hosting your resume on GitHub)

sass:
  sass_dir: _sass
  style: compressed

plugins:
 - jekyll-seo-tag

exclude : [
  "Gemfile",
  "Gemfile.lock",
  "node_modules",
  "vendor/bundle/",
  "vendor/cache/",
  "vendor/gems/",
  "vendor/ruby/",
  "lib/",
  "scripts/",
  "docker-compose.yml",
  ]
