# Site
repository: alibitarafan/alibitarafa.github.io


# Content configuration version
version: 2

# Personal info
name: Ali Bitarafan
title: Data Architect | Tech lead
email: ali.bitarafan@gmail.com


darkmode: false

# Social links
# twitter_username: jekyllrb
# github_username:  jekyll
# stackoverflow_username: "00000001"
# dribbble_username: jekyll
# facebook_username: jekyll
# flickr_username: jekyll
# instagram_username: jekyll
# linkedin_username: ali-bitarafan
# xing_username: jekyll
# pinterest_username: jekyll
# youtube_username: jekyll
# orcid_username: 0000-0000-0000-0000
# googlescholar_username: D847cGsAAAAJ

# Additional icon links
additional_links:
  - title: Solutions Architect Essentials 
    url: https://www.credential.net/d3129814-e75f-4da2-8f4b-3b832f80d43b
    image: /images/DB-SAE.png
  - title: Data Bricks Lakehouse fundamentals
    url: https://credentials.databricks.com/509bb8d3-46f5-41b1-93b6-3de27b59e65a
    image: /images/DB-lakehouse-fundamentals.png
  - title: Snow Pro - Core
    url: https://achieve.snowflake.com/ef5ec5c1-5437-4cba-ab60-aaa34e305c96
    image: /images/snowpro-core.png
  - title: Data Bricks Generative AI fundamentals
    url: https://credentials.databricks.com/1e574d0d-316b-4758-b07b-cf307336c474
    image: /images/DB-generative-ai.png
  - title: Azure Data Engineer Associate
    url: https://learn.microsoft.com/en-us/users/alibitarafan-22/credentials/e048254457227f56
    image: /images/microsoft-certified-associate-badge.svg
  - title: AWS Certified Solutions Architect - associate
    url: https://cp.certmetrics.com/amazon/en/public/verify/credential/9BCLZX7KW2RE1NKH
    image: /images/AWS-Solutions-Architect-Associate_badge.png

# - title: Link name
#   icon: Font Awesome brand icon name (eg. fab fa-twitter) (https://fontawesome.com/icons?d=gallery&s=brands&m=free)
#   url: Link url (eg. https://google.com)
# - title: another link
#   icon: font awesome brand icon name (eg. fab fa-twitter) (https://fontawesome.com/icons?d=gallery&s=brands&m=free)
#   url: Link url (eg. https://google.com)

# Google Analytics and Tag Manager
# Using more than one of these may cause issues with reporting
# gtm: "GTM-0000000"
# gtag: "UA-00000000-0"
# google_analytics: "UA-00000000-0"

# About Section
# about_title: About Me (Use this to override about section title)
about_profile_image: images/self-portrait.JPG
about_content: | # this will include new lines to allow paragraphs
    I spend my time 
      designing and implementing <mark>data warehouse</mark> solutions, managing <mark>data pipelines</mark>, and 
      implementing machine learning and analytical workloads. I have extensive experience with 
      <mark>Hadoop</mark>, <mark>Apache Spark</mark>, and <mark>Kafka</mark>, which I have utilized to build 
      scalable and efficient data processing systems. Additionally, I have worked with tools like 
      <mark>DBT</mark>, <mark>Databricks</mark>, and <mark>Snowflake</mark> to deliver robust data solutions. 
      Teamwork and continuous learning are values that resonate with me, allowing me to adapt to new technologies and methodologies. Effective communication 
      is one of my key strengths, enabling successful collaboration with business stakeholders to 
      translate their requirements into technical solutions. This combination of expertise in data 
      management, big data technologies, and strong communication abilities positions me well for roles 
      such as Data Engineering and Data Architect.

#  Write an awesome description about yourself here, this supports markdown, so you can add [links](http://foobar.com) and highlight things <mark>like this</mark>.
#
#  You can even add paragraphs by using empty lines like this and add anything else [markdown](https://www.markdownguide.org/getting-started#what-is-markdown) supports such as
#    - Lists
#    - Tables
# - <a href="google.com">Links</a>
# - Images ![alt text](/images/landscape-trees.jpg "Trees")


content:
  - title: Projects # Title for the section
    layout: list # Type of content section (list/text)
    content:
    - layout: left
      border: weak  # Value of `weak` will display a weak border below this item. # Any 
                    # other value (or no value) means no border will be displayed
      title: | 
        Frozed goods Warehouse Optimization (KONOIKE)
        AI solution - Storage and Picking
      # link: 
      # link_text: 
      quote: >
        Storage and Picking Optimization Project
      description: | # this will include new lines to allow paragraphs
        As a data architect and tech lead, I spearheaded the implementation of two AI solutions to optimize storage and picking processes. 
        Leveraging my expertise in <mark>Apache Spark</mark>, I designed scalable data pipelines 
        to process large datasets efficiently. Collaborated with business stakeholders to translate their requirements into technical solutions, 
        ensuring seamless integration with existing systems.
    - layout: left
      border: weak  # Value of `weak` will display a weak border below this item. # Any 
                    # other value (or no value) means no border will be displayed
      title: |
        Population Statistics
        data warehouse and pipeline desing  
      link: 
      link_text: 
      quote: >
        Optimising and modernising the data warehouse and data pipelines
      description: | # this will include new lines to allow paragraphs
        While maintaining the legacy data warehouse, I utilized <mark>Hadoop</mark> and <mark>Apache Spark</mark> to develop a modern data warehouse 
        in Microsoft SQL Server. Improved <mark>Data Vault</mark> models and implemented robust data pipelines to extract, transform, and load data 
        from diverse sources, ensuring high performance and scalability.  
    - layout: left
      border: weak
      title: |
        Finnish Forestery 
        safety analytics
      quopte: >
        Data warehouse, data pipeline and power bi dashboard design and development.
      description: |
        During the course of this project I developed three layer data transformation from legacy warehouse system to <mark>Snowflake</mark> and eventually building comprehensive analytical dashboard using <mark>power-bi</mark>.
        The data transformation layers (landing, staging, prod) was implemented using databricks and project was maintained in <mark>azure-devops</mark>.  
    - layout: left
      border: weak  # Value of `weak` will display a weak border below this item. # Any 
                    # other value (or no value) means no border will be displayed
      title: Forest bi-products manufacturer
      quote: >
        Optimizing the manufacturing reporting process
      description: | # this will include new lines to allow paragraphs
        Developed a <mark>Data Vault</mark> model to optimize the reporting process for manufacturing data. Designed and implemented 
        data pipelines using <mark>Kafka</mark> and <mark>Azure SQL Server</mark> to streamline data integration from legacy systems. 
        Leveraged cloud-based modern DataOps mechanisms to ensure scalability and efficiency. 
    - layout: left
      border: weak
      title: Steel manufacturer
      quote: >
        Data catalog and data governance of business analytics and industrial data
      description: | # this will include new lines to allow paragraphs
        Implemented <mark>Microsoft Purview</mark> to catalog and govern data assets, ensuring compliance with security and privacy regulations. 
        Collaborated with stakeholders to define data governance requirements and utilized <mark>Hadoop</mark> and <mark>Kafka</mark> to manage 
        large-scale industrial data efficiently.

    - layout: left
      border: weak
      title: Pharmceutical
      quote: >
        Data migration - on premise to Cloud
      description: | # this will include new lines to allow paragraphs
        Led the data migration strategy and execution, including designing and implementing AWS RDS Oracle procedures. 
        Utilized <mark>AWS Glue</mark> and <mark>Athena</mark> for data exploration and validation, and deployed CI/CD pipelines using <mark>AWS CDK</mark>. 
        Ensured seamless migration of large datasets while maintaining data integrity and performance.

  - title: Experience
    layout: list
    content:
      - layout: right
        border: weak
        title: Etteplan
        sub_title: Senior data engineer
        caption: April 2024 - Present
        link: https://www.etteplan.com
        quote: >
          Etteplan is a global company that specializes in AI as well as other industrial engineering services
        description: | # this will include new lines to allow paragraphs
          As a senior data engineer, I lead design and implementation of data workloads for AI solutions.
          using best practices in data modeling, data pipelines development operations 
          as well as data governance. I have worked on process automation with the goal of enabling automated data science, and data analytics jobs.
      - layout: right
        border: weak
        title: Lease Deal it
        sub_title: Senior Consultant
        caption: December 2023 - April 2024
        link: https://www.leasedealit.fi
        quote: >
          Lease deal it is a house of experts in the field of Information technology and business intelligence
        description: | # this will include new lines to allow paragraphs
          I was accountable for effectively communicating with clients to help them 
          draw project requirements and implement customized data warehouse solutions. 
          I applied the discussions in design and implementation of data pipelines. In practice this resulted in
          <mark>SQL procedures</mark>, resolving issues discovered within existing data pipelines, enhancing and updating existing data models,
          and optimizing existing data pipelines for improved efficiency.
      - layout: right
        title: Solita Oy
        border: weak
        sub_title: Data Engineer
        caption: April 2022 - December 2023
        link: https://www.solita.fi
        quote: >
          IT consulting company with a focus on data and analytics
        description: | # this will include new lines to allow paragraphs
          In my role as a Data Engineer, I developed pipelines and data warehouse solutions in both AWS and 
          Microsoft Azure platforms. Some of the tools I worked with were <mark>Azure DevOps</mark>, 
          Datalake, Data Vault, Databricks DE/MLops, Snowflake, <mark>AWS/Azure CLI</mark>, AWS Lambda/RDS/S3/Glue, 
          AWS/Azure datalake, and MS Purview.       
      - layout: right
        title: Kanava.to
        border: weak
        sub_title: Full Stack Developer
        caption: May 2021 - Mar 2022
        link: https://www.kanava.to
        quote: >
          Marketing and advertising company with a focus on digital media and application development
        description: | # this will include new lines to allow paragraphs
          I was part a small team focusing on building eCommerce applications and incorporated business
          intelligence features through proficiency in various programming languages 
          such as nodeJs, VueJs, and PHP, as well as databases like MySQL and PostgreSQL. 
          In this role, I am responsible for reporting progress to stakeholders, 
          engaging in development discussions and planning, facilitating database migration, 
          and executing web application renovations.
      - layout: right
        title: Centria University of Applied Sciences
        sub_title: R&D Engineer
        caption: Aug 2017 - Dec 2020
        link: https://www.centria.fi
        quote: >
          Higher education institution with focus on research and development
        description: | # this will include new lines to allow paragraphs
          I developed numerous automation tools and prepared quality data along with analytical reports that contributed to significant managerial decisions. I was granted the opportunity to share my learnings in business intelligence and ERP solutions like SAP BW/4HANA, SAP S/4HANA, and Analytical applications such as SAP Analytics Cloud with Bachelor's and Master's degree students. In this capacity, I practiced team coaching, tech-leadership, working extensively with <mark>SAP BW/4HANA</mark>, SAP S/4HANA, <mark>SAP Analytics Cloud</mark>, SAP Smart data integration, and employed Python and shell scripting for efficient problem-solving.
  - title: Education
    layout: list
    content:
      - layout: top-right
        border: weak
        title: Åbo Akademi
        sub_title: Master of Computer Science
        caption: 2025
        quote: >
          With focus on AI and machine learning, My major was Data science and I wrote my thesis on
          Automated <mark>knowledge extraction</mark> from text using Foundational Langugae models run locally on an average commercial computer.
        description: | # this will include new lines to allow paragraphs
        
      - layout: top-right
        title: Centria University of Applied Sciences
        sub_title: Bachelor of Information Technology
        caption: 2019
        quote: >
          I learned principles of software development and application engineering. I began to shift towards data engineering along the way and focused on writing my thesis about <mark>data integration</mark> in data warehousing and business intelligence applications. 
        description:
  
  # - title: A Little More About Me
  #   layout: text
  #   content: | # this will include new lines to allow paragraphs
  #     This is where you can write a little more about yourself. You could title this section **Interests** and include some of your other interests.

  #     Or you could title it **Skills** and write a bit more about things that make you more desirable, like *leadership* or *teamwork*

# Footer
footer_show_references: true
# references_title: References on request (Override references text)

# Build settings
# theme: modern-resume-theme (Use this is you are hosting your resume yourself)
# remote_theme: sproogen/modern-resume-theme (Use this if you are hosting your resume on GitHub)

sass:
  sass_dir: _sass
  style: compressed

plugins:
 - jekyll-seo-tag

exclude : [
  "Gemfile",
  "Gemfile.lock",
  "node_modules",
  "vendor/bundle/",
  "vendor/cache/",
  "vendor/gems/",
  "vendor/ruby/",
  "lib/",
  "scripts/",
  "docker-compose.yml",
  ]
